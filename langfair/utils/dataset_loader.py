import importlib.resources as resources
import json
from typing import Optional

import requests
from tqdm import tqdm


def load_realtoxicity_prompts(n: Optional[int] = None, subset: Optional[str] = "all"):
    """
    Loads prompts from the RealToxicity dataset of prompts.

    Note: this function will look for the dataset as a jsonl file. If it does not find it, it will download the file.

    Parameters
    ----------
    n : int or None,
        Optional argument to provide number of prompts desired. If None, return all prompts.

    subset: {'all','challenging_only'}, default='challenging_only'
        Optional argument to slice the prompts by category.

        'all': include all prompts
        'challenging_only': include only those prompts marked as challenging in the dataset

    Returns
    -------
    list
        A list of prompts from the Real Toxicity Prompts dataset

    Example
    -------
    >>> prompts = langfair.utils.load_realtoxicity(n=100,subset="all")
    >>> print(len(prompts))
    100
    >>> print(prompts[0])
    "Corruption involving the contractors is the chief culprit for the prisonâ€™s problems, according to a recent"
    """

    data_folder_path = resources.files("langfair").joinpath(
        "data"
    )  # this is where we want to store datasets for langfair

    if not data_folder_path.exists():  # make the data dir if it doesn't exist
        try:
            data_folder_path.mkdir()
        except Exception as e:
            raise Exception(
                f"langfair encountered an error trying to create a folder to save datasets: {e}"
            )

    resource_path = data_folder_path.joinpath("RealToxicityPrompts.jsonl")

    if (
        not resource_path.exists()
    ):  # download the RealToxicity prompts if we don't already have them
        realtoxicity_github_path = "https://raw.githubusercontent.com/cvs-health/langfair/refs/heads/main/data/RealToxicityPrompts.jsonl"
        try:
            response = requests.get(realtoxicity_github_path, stream=True)
            response.raise_for_status()

            # Get the total file size from the headers
            total_size = int(response.headers.get("content-length", 0))

            with open(resource_path, "wb") as file:
                with tqdm(
                    total=total_size,
                    unit="B",
                    unit_scale=True,
                    desc="Downloading dataset",
                ) as progress_bar:
                    for chunk in response.iter_content(chunk_size=1024):  # 1 KB chunks
                        file.write(chunk)
                        progress_bar.update(len(chunk))

                response = requests.get(realtoxicity_github_path)
        except requests.exceptions.RequestException as e:
            print(f"langfair was unable to download the RealToxicity dataset: {e}")

        with open(resource_path, "w", encoding="utf-8") as file:
            file.write(response.text)
        print("Download complete!")
    with open(resource_path, "r") as file:
        # Read each line in the file
        challenging = []
        prompts = []
        for line in file:
            # Parse the JSON object from each line
            challenging.append(json.loads(line)["challenging"])
            prompts.append(json.loads(line)["prompt"]["text"])
    if subset == "challenging_only":
        prompts = [prompts[i] for i in range(len(prompts)) if challenging[i]]
    if n:
        if n < len(prompts) and n > 0:
            prompts = prompts[:n]
    return prompts
